<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"  lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">

<title>Building a Neural Network from scratch | DomsDev Data-Science</title>



<link href="https://domsdev.github.io/Data-science-blog/index.xml" rel="alternate" type="application/rss+xml" title="DomsDev Data-Science">

<link rel="stylesheet" href="https://domsdev.github.io/Data-science-blog/css/style.css"><link rel='stylesheet' href='https://domsdev.github.io/Data-science-blog/css/custom.css'><link rel="apple-touch-icon" sizes="180x180" href="https://domsdev.github.io/Data-science-blog/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://domsdev.github.io/Data-science-blog/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://domsdev.github.io/Data-science-blog/favicon-16x16.png">
<link rel="manifest" href="https://domsdev.github.io/Data-science-blog/site.webmanifest">
<link rel="mask-icon" href="https://domsdev.github.io/Data-science-blog/safari-pinned-tab.svg" color="#5bbad5">
<link rel="canonical" href="https://domsdev.github.io/Data-science-blog/post/building_a_neural_network_from_scratch/">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
</head>
<body>
<section class="section">
  <div class="container">
    <nav id="nav-main" class="nav">
      <div id="nav-name" class="nav-left">
        <a id="nav-anchor" class="nav-item" href="https://domsdev.github.io/Data-science-blog/">
          <h1 id="nav-heading" class="title is-4">DomsDev Data-Science</h1>
        </a>
      </div>
      <div class="nav-right">
        <nav id="nav-items" class="nav-item level is-mobile"><a class="level-item" aria-label="linkedin" href='https://linkedin.com/in/dominique-pothin-dev/'
            target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <path stroke-width="1.8" d="m5.839218,4.101561c0,1.211972 -0.974141,2.194011 -2.176459,2.194011s-2.176459,-0.982039 -2.176459,-2.194011c0,-1.211094 0.974141,-2.194011 2.176459,-2.194011s2.176459,0.982917 2.176459,2.194011zm0.017552,3.94922l-4.388022,0l0,14.04167l4.388022,0l0,-14.04167zm7.005038,0l-4.359939,0l0,14.04167l4.360816,0l0,-7.370999c0,-4.098413 5.291077,-4.433657 5.291077,0l0,7.370999l4.377491,0l0,-8.89101c0,-6.915523 -7.829986,-6.66365 -9.669445,-3.259423l0,-1.891237z"/>
    
  </svg></i>
            </span>
          </a><a class="level-item" aria-label="github" href='https://github.com/Domsdev'
            target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"/>
    
  </svg></i>
            </span>
          </a><a class="level-item" aria-label="email" href='mailto:domsdev.sender@gmail.com'
            target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"/>
    <polyline points="22,6 12,13 2,6"/>
    
  </svg></i>
            </span>
          </a></nav>
      </div>
    </nav>

    <nav class="nav">
      

      
    </nav>

  </div>
  <script src="https://domsdev.github.io/Data-science-blog/js/navicon-shift.js"></script>
</section>
<section class="section">
  <div class="container">
    <div class="subtitle tags is-6 is-pulled-right">
      
      
<a class="subtitle is-6" href="https://domsdev.github.io/Data-science-blog/tags/neural-network/">#Neural Network</a>



  
  | <a class="subtitle is-6" href="https://domsdev.github.io/Data-science-blog/tags/from-scratch/">#From Scratch</a>
  


      
    </div>
    <h2 class="subtitle is-6">October 13, 2020</h2>
    <h1 class="title">Building a Neural Network from scratch</h1>
    
    <div class="content">
      <!-- raw HTML omitted -->
<h2 id="objective">Objective:</h2>
<p>The goal here is to deal with the XOr classification problem that can&rsquo;t be solved with classical logistic regression models. The XOr, or “exclusive or” problem is a classic problem that can be solved with Artificial Neural Network to predict the outputs of XOr logic gates given two binary inputs. The XOr function returns a true value if the two inputs are not equal and a false value if they are equal.<!-- raw HTML omitted --></p>
<p>I will apply a such <strong>ANN built from scratch</strong> to a custom dataset generated for XOr binary classification.</p>
<p><img src="https://domsdev.github.io/Data-science-blog/images/Building_a_Neural_Network_from_scratch/2020-10-13/xor_example.png#c" alt="png"></p>
<hr>
<h2 id="table-of-contents">Table of contents</h2>
<ol>
<li><a href="#data">Generating Dataset</a></li>
<li><a href="#dataprep">Data Preprocessing</a></li>
<li><a href="#background">ANN - Model and background</a></li>
<li><a href="#nn_scratch">ANN from Scratch</a></li>
<li><a href="#evaluation">Model evaluation</a></li>
<li><a href="#comparison">Comparison with sklearn MLPClassifier model</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ol>
<h3 id="importing-libraries">Importing libraries</h3>
<div class="highlight"><pre style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#8b008b;font-weight:bold">import</span> <span style="color:#008b45;text-decoration:underline">numpy</span> <span style="color:#8b008b;font-weight:bold">as</span> <span style="color:#008b45;text-decoration:underline">np</span>
<span style="color:#8b008b;font-weight:bold">import</span> <span style="color:#008b45;text-decoration:underline">matplotlib.pyplot</span> <span style="color:#8b008b;font-weight:bold">as</span> <span style="color:#008b45;text-decoration:underline">plt</span>
<span style="color:#8b008b;font-weight:bold">from</span> <span style="color:#008b45;text-decoration:underline">sklearn.model_selection</span> <span style="color:#8b008b;font-weight:bold">import</span> train_test_split
<span style="color:#8b008b;font-weight:bold">from</span> <span style="color:#008b45;text-decoration:underline">sklearn.preprocessing</span> <span style="color:#8b008b;font-weight:bold">import</span> MinMaxScaler
<span style="color:#8b008b;font-weight:bold">from</span> <span style="color:#008b45;text-decoration:underline">sklearn.metrics</span> <span style="color:#8b008b;font-weight:bold">import</span> classification_report
<span style="color:#8b008b;font-weight:bold">from</span> <span style="color:#008b45;text-decoration:underline">sklearn.metrics</span> <span style="color:#8b008b;font-weight:bold">import</span> confusion_matrix
<span style="color:#8b008b;font-weight:bold">import</span> <span style="color:#008b45;text-decoration:underline">seaborn</span> <span style="color:#8b008b;font-weight:bold">as</span> <span style="color:#008b45;text-decoration:underline">sns</span>
<span style="color:#8b008b;font-weight:bold">from</span> <span style="color:#008b45;text-decoration:underline">tqdm</span> <span style="color:#8b008b;font-weight:bold">import</span> tqdm
%matplotlib inline
</code></pre></div><h2 id="1generating-dataseta-iddataa">1.Generating Dataset<!-- raw HTML omitted --><!-- raw HTML omitted --></h2>
<p><strong>Let&rsquo;s generate our dataset using Numpy XOr logical function</strong></p>
<div class="highlight"><pre style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">np.random.seed(<span style="color:#b452cd">50</span>)
n_samples = <span style="color:#b452cd">1000</span>

<span style="color:#228b22"># Generating n_samples from Normal distribution N(mu, sigma^2):</span>
mu, sigma = <span style="color:#b452cd">2</span>, <span style="color:#b452cd">3</span>
X = sigma * np.random.randn(n_samples, <span style="color:#b452cd">2</span>) + mu
y = np.logical_xor(X[:, <span style="color:#b452cd">0</span>] &gt; <span style="color:#b452cd">1</span>, X[:, <span style="color:#b452cd">1</span>] &gt; <span style="color:#b452cd">2</span>).astype(<span style="color:#658b00">int</span>)

<span style="color:#228b22"># Applying some distorsion to our distribution:</span>
<span style="color:#228b22"># Will be more challenging to our models</span>
transformation = [[<span style="color:#b452cd">12</span>, <span style="color:#b452cd">7</span>], [-<span style="color:#b452cd">5</span>, <span style="color:#b452cd">4</span>]]
X = np.dot(X, transformation)

<span style="color:#8b008b;font-weight:bold">print</span>(<span style="color:#cd5555">&#39;X shape:&#39;</span>, X.shape)
<span style="color:#8b008b;font-weight:bold">print</span>(<span style="color:#cd5555">&#39;y shape:&#39;</span>, y.shape)
</code></pre></div><pre><code>X shape: (1000, 2)
y shape: (1000,)
</code></pre>
<p>Here under a brief look on variables generated:</p>
<div class="highlight"><pre style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#8b008b;font-weight:bold">for</span> i <span style="color:#8b008b">in</span> <span style="color:#658b00">range</span>(<span style="color:#b452cd">4</span>):
    <span style="color:#8b008b;font-weight:bold">print</span>(f<span style="color:#cd5555">&#34;Class:{y[i]} | Coordinates:{X[i]}&#34;</span>)
</code></pre></div><pre><code>Class:0 | Coordinates:[-41.70801186 -11.13912552]
Class:0 | Coordinates:[13.61528402 -8.61446272]
Class:1 | Coordinates:[71.98104254 45.93008284]
Class:1 | Coordinates:[-30.15090773  18.45335933]
</code></pre>
<h3 id="data-visualization">Data visualization</h3>
<div class="highlight"><pre style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">plt.rcParams.update({<span style="color:#cd5555">&#39;axes.facecolor&#39;</span>:<span style="color:#cd5555">&#39;#f8fafc&#39;</span>})
%matplotlib inline

plt.figure(figsize=(<span style="color:#b452cd">12</span>,<span style="color:#b452cd">8</span>))

plt.scatter(X[y == <span style="color:#b452cd">0</span>, <span style="color:#b452cd">0</span>], X[y == <span style="color:#b452cd">0</span>, <span style="color:#b452cd">1</span>], label= <span style="color:#cd5555">&#39;Class 0&#39;</span>,
            c= <span style="color:#cd5555">&#39;darkorange&#39;</span>, marker= <span style="color:#cd5555">&#39;o&#39;</span>, edgecolor=<span style="color:#cd5555">&#39;k&#39;</span>, alpha=<span style="color:#b452cd">0.8</span>, s=<span style="color:#b452cd">80</span>)
plt.scatter(X[y == <span style="color:#b452cd">1</span>, <span style="color:#b452cd">0</span>], X[y == <span style="color:#b452cd">1</span>, <span style="color:#b452cd">1</span>], label= <span style="color:#cd5555">&#39;Class 1&#39;</span>,
            c= <span style="color:#cd5555">&#39;mediumorchid&#39;</span>, marker= <span style="color:#cd5555">&#39;o&#39;</span>, edgecolor=<span style="color:#cd5555">&#39;k&#39;</span>, alpha=<span style="color:#b452cd">0.8</span>, s=<span style="color:#b452cd">80</span>)
plt.xlabel(<span style="color:#cd5555">&#34;$$x_1$$&#34;</span>, fontsize=<span style="color:#b452cd">16</span>)
plt.ylabel(<span style="color:#cd5555">&#34;$$x_2$$&#34;</span>, fontsize=<span style="color:#b452cd">16</span>)
plt.title(<span style="color:#cd5555">&#39;Scatter plot of the data&#39;</span>, fontsize=<span style="color:#b452cd">18</span>)
plt.legend(loc=<span style="color:#cd5555">&#34;upper left&#34;</span>, facecolor=<span style="color:#cd5555">&#39;white&#39;</span>)

plt.show()
</code></pre></div><p><img src="https://domsdev.github.io/Data-science-blog/images/Building_a_Neural_Network_from_scratch/2020-10-13/output_14_0.png#c" alt="png"></p>
<p>This is the kind of dataset where it is not possible to find a simple linear or non-linear frontier of separation between the two classes, and this is why an artificial neural network is helpfull in this case.</p>
<h2 id="2data-preprocessing-a-iddataprepa">2.Data Preprocessing <!-- raw HTML omitted --><!-- raw HTML omitted --></h2>
<h3 id="splitting-dataset">Splitting Dataset</h3>
<p>Dataset is splitted into training set with 80% of the samples and test set with the remaining 20%, using &ldquo;stratified sampling&rdquo; to ensure that relative class frequencies is approximately preserved in each train and test fold.</p>
<div class="highlight"><pre style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size= <span style="color:#b452cd">0.2</span>,
    stratify= y,
    random_state= <span style="color:#b452cd">50</span>)

<span style="color:#8b008b;font-weight:bold">print</span>(<span style="color:#cd5555">&#39;X_train shape:&#39;</span>, X_train.shape)
<span style="color:#8b008b;font-weight:bold">print</span>(<span style="color:#cd5555">&#39;X_test shape:&#39;</span>, X_test.shape)
</code></pre></div><pre><code>X_train shape: (800, 2)
X_test shape: (200, 2)
</code></pre>
<h3 id="data-scaling">Data Scaling</h3>
<p>Features are transformed to the given range [0,1] by scaling:</p>
<p>$$X_{scaled} = X_{std} * (max - min) + min$$<!-- raw HTML omitted -->
Where:
$$X_{std} = (X - X_{min}) / (X_{max} - X_{min})$$</p>
<div class="highlight"><pre style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">scaler = MinMaxScaler(feature_range=(<span style="color:#b452cd">0</span>, <span style="color:#b452cd">1</span>))

X_train = scaler.fit_transform(X_train) <span style="color:#228b22"># fit and transform training set</span>
X_test  = scaler.transform(X_test)      <span style="color:#228b22"># transform testing set with &#34;Scaler&#34; fited</span>
</code></pre></div><h2 id="3ann---model-and-backgrounda-idbackgrounda">3.ANN - Model and background<!-- raw HTML omitted --><!-- raw HTML omitted --></h2>
<h4 id="artificial-neuron">Artificial Neuron</h4>
<p>As a reminder and support visualization for background equations, this is the structure of an artificial neuron.</p>
<p><img src="https://domsdev.github.io/Data-science-blog/images/Building_a_Neural_Network_from_scratch/2020-10-13/perceptron.png#c" alt="png"></p>
<h4 id="artificial-neural-network-model-definition">Artificial Neural Network Model definition</h4>
<p>This is the model&rsquo;s structure that I&rsquo;m going to build from scratch:</p>
<p><img src="https://domsdev.github.io/Data-science-blog/images/Building_a_Neural_Network_from_scratch/2020-10-13/neural_network_model.png#c" alt="png"></p>
<h4 id="step-1-forward-propagation">Step 1: Forward Propagation</h4>
<p>$$1^{st} \hspace{2mm}layer\hspace{2mm}summation: Z^{[1]} = W^{[1]}X + b^{[1]}$$</p>
<p>$$1^{st} \hspace{2mm}layer\hspace{2mm}activation: A^{[1]} = g^{[1]}{(Z^{[1]})}$$</p>
<p>$$2^{nd} \hspace{2mm}layer\hspace{2mm}summation: Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$$</p>
<p>$$2^{nd} \hspace{2mm}layer\hspace{2mm}activation: A^{[2]} = g^{[2]}{( Z^{[2]} )}$$</p>
<p>Where $$X = [x_1, x_2, &hellip;, x_N]$$ is the data matrix with N samples, and the $$g^{[i]}$$ are the activation functions  Sigmoid: $$\sigma(x) = \frac{1}{1 + exp(-x)}$$</p>
<h4 id="step-2-cross-entropy-loss">Step 2: Cross entropy loss</h4>
<p>$$J = \frac{1}{N} \sum\limits_{i = 0}^{N} \large{(} \small - y^{(i)}\log\left(a^{[2] (i)}\right) - (1-y^{(i)})\log\left(1- a^{[2] (i)}\right) \large{)}$$</p>
<h4 id="step-3-back-propagation">Step 3: Back Propagation</h4>
<p>$$dZ^{[2]} = A^{[2]} - Y$$</p>
<p>$$dW^{[2]} = \frac{1}{N} dZ^{[2]} {A^{[1]}}^T$$</p>
<p>$$db^{[2]} = \frac{1}{N} \sum dZ^{[2]} $$</p>
<p>$$dZ^{[1]} = ({W^{[2]}}^T  dZ^{[2]}) * {g^{[1]'}} (Z^{[1]})$$</p>
<p>$$dW^{[1]} = \frac{1}{N} dZ^{[1]} {X}^T$$</p>
<p>$$db^{[1]} = \frac{1}{N} \sum dZ^{[1]} $$</p>
<!-- raw HTML omitted -->
<p>Where $$ \frac{dJ}{dZ^{[2]}} = \frac{dJ}{dA^{[2]}} \frac{dA^{[2]}}{dZ^{[2]}} \implies {dZ^{[2]}} = dA^{[2]} {g^{[2]'}} (Z^{[2]}) = A^{[2]} - Y $$</p>
<p>$$ \frac{dJ}{dW^{[2]}}  = \frac{dJ}{dZ^{[2]}} \frac{dZ^{[2]}}{dW^{[2]}} \implies {dW^{[2]}} = \frac{1}{N} dZ^{[2]} {A^{[1]}}^T$$</p>
<p>With $${g'(z)} = \frac{d}{dz} g(z) $$</p>
<p>Knwowing that for sigmoid fuction we get: $${\sigma}'(z) = \sigma(z) (1 - \sigma(z))$$</p>
<h2 id="4-ann-from-scratch-a-idnn_scratcha">4. ANN from Scratch <!-- raw HTML omitted --><!-- raw HTML omitted --></h2>
<h3 id="lets-build-step-by-step-a-neural-network-python-class">Let&rsquo;s build step by step a Neural Network python class</h3>
<div class="highlight"><pre style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#8b008b;font-weight:bold">class</span> <span style="color:#008b45;font-weight:bold">Artificial_Neural_Network</span>:
    
    <span style="color:#228b22"># Step 1: Implementing initialization function with default parameters</span>
    <span style="color:#8b008b;font-weight:bold">def</span> __init__(self, n_x = <span style="color:#b452cd">2</span>, n_h = <span style="color:#b452cd">3</span>, n_y = <span style="color:#b452cd">1</span>, lr= <span style="color:#b452cd">0.1</span>, num_iter= <span style="color:#b452cd">1000</span>):
        self.W1 = np.random.randn(n_h, n_x)
        self.b1 = np.random.randn(n_h, <span style="color:#b452cd">1</span>)
        self.W2 = np.random.randn(n_y, n_h)
        self.b2 = np.random.randn(n_y, <span style="color:#b452cd">1</span>)
        self.lr = lr
        self.num_iter = num_iter
    
    <span style="color:#228b22"># Step 2: Implementing sigmoid function and its differential</span>
    <span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">sigmoid</span>(self, t):
        <span style="color:#8b008b;font-weight:bold">return</span> <span style="color:#b452cd">1</span>/(<span style="color:#b452cd">1</span> + np.exp(-t))
    
    <span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">sigmoid_derivative</span>(self, t):
        <span style="color:#8b008b;font-weight:bold">return</span> self.sigmoid(t)*(<span style="color:#b452cd">1</span> - self.sigmoid(t))
    
    <span style="color:#228b22"># Step 3: Implementing forward propagation</span>
    <span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">feedforward</span>(self, X):
        self.Z1 = self.W1.dot(X.T) + self.b1
        self.A1 = self.sigmoid(self.Z1)
        self.Z2 = self.W2.dot(self.A1) + self.b2
        self.A2 = self.sigmoid(self.Z2)
        <span style="color:#8b008b;font-weight:bold">return</span> self.A2
    
    <span style="color:#228b22"># Step 4: Implementing backward backpropagation</span>
    <span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">backpropagation</span>(self, X, y):
        N = X.shape[<span style="color:#b452cd">0</span>] <span style="color:#228b22"># Number of samples</span>
        dZ2 = self.A2 - y.T
        self.dW2 = <span style="color:#b452cd">1</span>/N * dZ2.dot(self.A1.T)
        self.db2 = <span style="color:#b452cd">1</span>/N * dZ2.sum(axis=<span style="color:#b452cd">1</span>, keepdims= True)
        dZ1 = self.W2.T.dot(dZ2) * self.sigmoid_derivative(self.Z1)
        self.dW1 = <span style="color:#b452cd">1</span>/N * dZ1.dot(X)
        self.db1 = <span style="color:#b452cd">1</span>/N * dZ1.sum(axis=<span style="color:#b452cd">1</span>, keepdims= True)
        
    <span style="color:#228b22">#Step 5: Implementing weights and bias update</span>
    <span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">update_parameters</span>(self):
        self.W1 -= self.lr * self.dW1
        self.b1 -= self.lr * self.db1
        self.W2 -= self.lr * self.dW2
        self.b2 -= self.lr * self.db2
    
    <span style="color:#228b22"># Step 6: Implementing cross entropy loss</span>
    <span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">loss</span>(self,y, y_hat):
        <span style="color:#8b008b;font-weight:bold">return</span> (-y * np.log(y_hat) - (<span style="color:#b452cd">1</span>-y) * np.log(<span style="color:#b452cd">1</span> - y_hat)).mean()
    
    <span style="color:#228b22"># Step 7: Implementing the fit function (to train the model on data)</span>
    <span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">fit</span>(self, X, y):
        loss_history = []
        accuracy_history = []
        
        <span style="color:#8b008b;font-weight:bold">for</span> <span style="color:#658b00">iter</span> <span style="color:#8b008b">in</span> tqdm(<span style="color:#658b00">range</span>(self.num_iter)):
            y_hat = self.feedforward(X)
            loss_history.append(self.loss(y, y_hat))
            self.backpropagation(X, y)
            self.update_parameters()
            
            y_hat = y_hat.round().reshape(-<span style="color:#b452cd">1</span>)
            accuracy_history.append((y_hat == y).mean())
            
        <span style="color:#8b008b;font-weight:bold">return</span> loss_history, accuracy_history
    
    <span style="color:#228b22"># Step 8: Implementing probability estimation calculation</span>
    <span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">predict_proba</span>(self, X):
        <span style="color:#8b008b;font-weight:bold">return</span> self.feedforward(X) <span style="color:#228b22"># return a probability</span>
    
    <span style="color:#228b22"># Step 9: Implementing Class label prediction</span>
    <span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">predict</span>(self, X):
        <span style="color:#8b008b;font-weight:bold">return</span> self.predict_proba(X).round().reshape(-<span style="color:#b452cd">1</span>)
        <span style="color:#228b22"># if predict_proba &gt; 0.5  -&gt; will return 1</span>
        <span style="color:#228b22"># if predict_proba &lt;= 0.5 -&gt; will return 0</span>
</code></pre></div><h3 id="now-we-can-define-some-parameters-and-hyper-parameters-">Now we can define some parameters and hyper parameters &hellip;</h3>
<div class="highlight"><pre style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">N_INPUT_LAYER = X.shape[<span style="color:#b452cd">1</span>]  <span style="color:#228b22"># nb of neurons of input layer (number of features)</span>
N_HIDDEN_LAYER1 = <span style="color:#b452cd">3</span>         <span style="color:#228b22"># nb of neurons of intermediate layer</span>
N_OUTPUT_LAYER = <span style="color:#b452cd">1</span>          <span style="color:#228b22"># nb of neurons of output layer (gives predicted class 0 or 1)</span>

LEARNING_RATE = <span style="color:#b452cd">0.2</span>
ITERATIONS = <span style="color:#b452cd">80000</span>
</code></pre></div><h3 id="-create-an-instance-of-our-neural-network-class">&hellip; create an instance of our Neural Network class</h3>
<div class="highlight"><pre style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">ANN_model = Artificial_Neural_Network(
    n_x = N_INPUT_LAYER,
    n_h = N_HIDDEN_LAYER1,
    n_y = N_OUTPUT_LAYER,
    lr = LEARNING_RATE,
    num_iter = ITERATIONS)
</code></pre></div><h3 id="fit-the-model-over-training-set--and-finally-evaluate-our-model-">Fit the model over training set &hellip; and finally evaluate our model !</h3>
<div class="highlight"><pre style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">loss, accuracy = ANN_model.fit(X_train, y_train)
</code></pre></div><pre><code>100%|██████████| 80000/80000 [00:32&lt;00:00, 2473.67it/s]
</code></pre>
<h2 id="5model-evaluationa-idevaluationa">5.Model evaluation<!-- raw HTML omitted --><!-- raw HTML omitted --></h2>
<h3 id="lets-have-a-look-on-our-accuracy-and-loss-evolution">Let&rsquo;s have a look on our accuracy and loss evolution</h3>
<div class="highlight"><pre style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">plt.figure(figsize=(<span style="color:#b452cd">15</span>,<span style="color:#b452cd">5</span>))

plt.subplot(<span style="color:#b452cd">121</span>)
plt.plot(accuracy, c= <span style="color:#cd5555">&#39;yellowgreen&#39;</span>, linewidth=<span style="color:#b452cd">3</span>)
plt.xlabel(<span style="color:#cd5555">&#39;Iterations&#39;</span>, fontsize = <span style="color:#b452cd">12</span>), plt.ylabel(<span style="color:#cd5555">&#39;Accuracy&#39;</span>, fontsize = <span style="color:#b452cd">12</span>)
plt.title(<span style="color:#cd5555">&#39;Accuracy&#39;</span>, fontsize = <span style="color:#b452cd">15</span>)

plt.subplot(<span style="color:#b452cd">122</span>)
plt.plot(loss, c= <span style="color:#cd5555">&#39;yellowgreen&#39;</span>, linewidth=<span style="color:#b452cd">3</span>)
plt.xlabel(<span style="color:#cd5555">&#39;Iterations&#39;</span>, fontsize = <span style="color:#b452cd">12</span>), plt.ylabel(<span style="color:#cd5555">&#39;Loss&#39;</span>, fontsize = <span style="color:#b452cd">12</span>)
plt.title(<span style="color:#cd5555">&#39;Loss&#39;</span>, fontsize = <span style="color:#b452cd">15</span>)

plt.show()
</code></pre></div><p><img src="https://domsdev.github.io/Data-science-blog/images/Building_a_Neural_Network_from_scratch/2020-10-13/output_48_0.png#c" alt="png"></p>
<p>Note how many many iterations the model needs in order to converge.</p>
<h3 id="now-we-can-check-the-models-accuracy-on-training-set-">Now we can check the model&rsquo;s accuracy on training set &hellip;</h3>
<div class="highlight"><pre style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">y_train_pred = ANN_model.predict(X_train)
</code></pre></div><div class="highlight"><pre style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">train_accuracy = (y_train_pred == y_train).mean()
<span style="color:#8b008b;font-weight:bold">print</span>(f<span style="color:#cd5555">&#39;Accuracy over training set = {100 * train_accuracy:.2f}%&#39;</span>)
</code></pre></div><pre><code>Accuracy over training set = 91.50%
</code></pre>
<h3 id="-and-check-the-accuracy-of-our-model-on-the-testing-set">&hellip; And check the accuracy of our model on the testing set</h3>
<div class="highlight"><pre style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">y_test_pred = ANN_model.predict(X_test)
</code></pre></div><div class="highlight"><pre style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">test_accuracy = (y_test_pred == y_test).mean()
<span style="color:#8b008b;font-weight:bold">print</span>(f<span style="color:#cd5555">&#39;Accuracy over testing set = {100 * test_accuracy:.2f}%&#39;</span>)
</code></pre></div><pre><code>Accuracy over testing set = 91.00%
</code></pre>
<p>We obtain a great accuracy on test data !!</p>
<h3 id="checking-classification-capabilities-with-the-classification-report-">Checking classification capabilities with the Classification report &hellip;</h3>
<div class="highlight"><pre style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">report = classification_report(y_test, y_test_pred)
<span style="color:#8b008b;font-weight:bold">print</span>(report)
</code></pre></div><pre><code>              precision    recall  f1-score   support

           0       0.90      0.93      0.91       103
           1       0.92      0.89      0.91        97

    accuracy                           0.91       200
   macro avg       0.91      0.91      0.91       200
weighted avg       0.91      0.91      0.91       200
</code></pre>
<p>We get pretty good classification scores here !</p>
<h3 id="-and-the-confusion-matrix">&hellip; And the Confusion Matrix</h3>
<div class="highlight"><pre style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">cf_matrix = confusion_matrix(y_test, y_test_pred)
</code></pre></div><div class="highlight"><pre style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">plt.figure(figsize=(<span style="color:#b452cd">10</span>,<span style="color:#b452cd">6</span>))

names = [<span style="color:#cd5555">&#39;True Neg&#39;</span>,<span style="color:#cd5555">&#39;False Pos&#39;</span>,<span style="color:#cd5555">&#39;False Neg&#39;</span>,<span style="color:#cd5555">&#39;True Pos&#39;</span>]
counts = [f<span style="color:#cd5555">&#39;{value:0.0f}&#39;</span> <span style="color:#8b008b;font-weight:bold">for</span> value <span style="color:#8b008b">in</span> cf_matrix.flatten()]
percentages = [<span style="color:#cd5555">&#39;{0:.1%}&#39;</span>.format(value) <span style="color:#8b008b;font-weight:bold">for</span> value <span style="color:#8b008b">in</span>
                     cf_matrix.flatten()/np.sum(cf_matrix)]
labels = [f<span style="color:#cd5555">&#39;{l1}</span><span style="color:#cd5555">\n</span><span style="color:#cd5555">{l2}</span><span style="color:#cd5555">\n</span><span style="color:#cd5555">{l3}&#39;</span> <span style="color:#8b008b;font-weight:bold">for</span> l1, l2, l3 <span style="color:#8b008b">in</span>
                           <span style="color:#658b00">zip</span>(names, counts, percentages)]
labels = np.asarray(labels).reshape(<span style="color:#b452cd">2</span>,<span style="color:#b452cd">2</span>)

sns.heatmap(cf_matrix, annot=labels, annot_kws={<span style="color:#cd5555">&#34;fontsize&#34;</span>:<span style="color:#b452cd">14</span>}, fmt=<span style="color:#cd5555">&#39;&#39;</span>, cmap=<span style="color:#cd5555">&#39;Blues&#39;</span>)
plt.ylabel(<span style="color:#cd5555">&#39;True labels&#39;</span>, size=<span style="color:#b452cd">16</span>)
plt.xlabel(<span style="color:#cd5555">&#39;Predicted labels&#39;</span>, size=<span style="color:#b452cd">16</span>)

plt.show()
</code></pre></div><p><img src="https://domsdev.github.io/Data-science-blog/images/Building_a_Neural_Network_from_scratch/2020-10-13/output_62_0.png#c" alt="png"></p>
<p>Over the 200 samples of the test set, we get only 7 samples with a Type I error (false positive) and only 11 samples with a Type II error (false negative).<!-- raw HTML omitted --></p>
<p><strong>We can say that our model has good classification capabilities regarding the given data.</strong></p>
<h2 id="6comparison-with-sklearn-mlpclassifier-modela-idcomparisona">6.Comparison with sklearn MLPClassifier model<!-- raw HTML omitted --><!-- raw HTML omitted --></h2>
<p>Fisrt I define hyperparameters, choosing same parameters that I used for the custom model, except the number of iterations set to 500 &hellip; that will be enough with this model.</p>
<div class="highlight"><pre style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">N_HIDDEN_LAYER1 = <span style="color:#b452cd">3</span> <span style="color:#228b22"># nb of neurons of intermediate layer</span>

LEARNING_RATE = <span style="color:#b452cd">0.2</span>
ITERATIONS = <span style="color:#b452cd">500</span>
</code></pre></div><p>Then I define a Multi Layer Perceptron Classifier model using Sklearn library&hellip;</p>
<div class="highlight"><pre style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#8b008b;font-weight:bold">from</span> <span style="color:#008b45;text-decoration:underline">sklearn.neural_network</span> <span style="color:#8b008b;font-weight:bold">import</span> MLPClassifier
MLP_model = MLPClassifier(
    hidden_layer_sizes=(N_HIDDEN_LAYER1),
    activation=<span style="color:#cd5555">&#39;logistic&#39;</span>,
    solver=<span style="color:#cd5555">&#39;adam&#39;</span>,
    learning_rate_init=LEARNING_RATE,
    max_iter=ITERATIONS)
</code></pre></div><p>&hellip; and fit the model to the data.</p>
<div class="highlight"><pre style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">MLP_model.fit(X_train, y_train)
</code></pre></div><pre><code>MLPClassifier(activation='logistic', hidden_layer_sizes=3,
              learning_rate_init=0.2, max_iter=500)
</code></pre>
<p>Let&rsquo;s have a look on the learning curve:</p>
<div class="highlight"><pre style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">plt.rcParams.update({<span style="color:#cd5555">&#39;axes.facecolor&#39;</span>:<span style="color:#cd5555">&#39;#f8fafc&#39;</span>})
plt.figure(figsize=(<span style="color:#b452cd">8</span>,<span style="color:#b452cd">5</span>))

plt.plot(MLP_model.loss_curve_, c= <span style="color:#cd5555">&#39;yellowgreen&#39;</span>, linewidth=<span style="color:#b452cd">3</span>)
plt.xlabel(<span style="color:#cd5555">&#39;Iterations&#39;</span>, fontsize = <span style="color:#b452cd">12</span>), plt.ylabel(<span style="color:#cd5555">&#39;Loss&#39;</span>, fontsize = <span style="color:#b452cd">12</span>)
plt.title(<span style="color:#cd5555">&#39;Loss&#39;</span>, fontsize = <span style="color:#b452cd">15</span>)

plt.show()
</code></pre></div><p><img src="https://domsdev.github.io/Data-science-blog/images/Building_a_Neural_Network_from_scratch/2020-10-13/output_72_0.png#c" alt="png"></p>
<p>Clearly, this model converges much faster than our ANN model built from scratch ! It needs around 200 iterations to do so. This is due to the efficiency of the ADAM optimization algorithm which is much faster than our loss optimizer.</p>
<div class="highlight"><pre style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#8b008b;font-weight:bold">from</span> <span style="color:#008b45;text-decoration:underline">sklearn.metrics</span> <span style="color:#8b008b;font-weight:bold">import</span> accuracy_score

y_train_pred = MLP_model.predict(X_train)
y_test_pred = MLP_model.predict(X_test)

<span style="color:#8b008b;font-weight:bold">print</span>(<span style="color:#cd5555">&#34;Training Accuracy: {0:.2f}&#34;</span>.format(accuracy_score(y_train, y_train_pred)))
<span style="color:#8b008b;font-weight:bold">print</span>(<span style="color:#cd5555">&#34;Testing Accuracy: {0:.2f}&#34;</span>.format(accuracy_score(y_test, y_test_pred)))
</code></pre></div><pre><code>Training Accuracy: 0.92
Testing Accuracy: 0.92
</code></pre>
<p>Good accuracy on test set !</p>
<div class="highlight"><pre style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">report = classification_report(y_test, y_test_pred)
<span style="color:#8b008b;font-weight:bold">print</span>(report)
</code></pre></div><pre><code>              precision    recall  f1-score   support

           0       0.91      0.92      0.92       103
           1       0.92      0.91      0.91        97

    accuracy                           0.92       200
   macro avg       0.92      0.91      0.91       200
weighted avg       0.92      0.92      0.91       200
</code></pre>
<p>Classification report show good scores, comparable to those obtained with the custom model.</p>
<div class="highlight"><pre style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">cf_matrix = confusion_matrix(y_test, y_test_pred)

plt.figure(figsize=(<span style="color:#b452cd">10</span>,<span style="color:#b452cd">6</span>))

names = [<span style="color:#cd5555">&#39;True Neg&#39;</span>,<span style="color:#cd5555">&#39;False Pos&#39;</span>,<span style="color:#cd5555">&#39;False Neg&#39;</span>,<span style="color:#cd5555">&#39;True Pos&#39;</span>]
counts = [f<span style="color:#cd5555">&#39;{value:0.0f}&#39;</span> <span style="color:#8b008b;font-weight:bold">for</span> value <span style="color:#8b008b">in</span> cf_matrix.flatten()]
percentages = [<span style="color:#cd5555">&#39;{0:.1%}&#39;</span>.format(value) <span style="color:#8b008b;font-weight:bold">for</span> value <span style="color:#8b008b">in</span>
                     cf_matrix.flatten()/np.sum(cf_matrix)]
labels = [f<span style="color:#cd5555">&#39;{l1}</span><span style="color:#cd5555">\n</span><span style="color:#cd5555">{l2}</span><span style="color:#cd5555">\n</span><span style="color:#cd5555">{l3}&#39;</span> <span style="color:#8b008b;font-weight:bold">for</span> l1, l2, l3 <span style="color:#8b008b">in</span>
                           <span style="color:#658b00">zip</span>(names, counts, percentages)]
labels = np.asarray(labels).reshape(<span style="color:#b452cd">2</span>,<span style="color:#b452cd">2</span>)

sns.heatmap(cf_matrix, annot=labels, annot_kws={<span style="color:#cd5555">&#34;fontsize&#34;</span>:<span style="color:#b452cd">14</span>}, fmt=<span style="color:#cd5555">&#39;&#39;</span>, cmap=<span style="color:#cd5555">&#39;Blues&#39;</span>)
plt.ylabel(<span style="color:#cd5555">&#39;True labels&#39;</span>, size=<span style="color:#b452cd">16</span>)
plt.xlabel(<span style="color:#cd5555">&#39;Predicted labels&#39;</span>, size=<span style="color:#b452cd">16</span>)

plt.show()
</code></pre></div><p><img src="https://domsdev.github.io/Data-science-blog/images/Building_a_Neural_Network_from_scratch/2020-10-13/output_78_0.png#c" alt="png"></p>
<p>Same comment for this confusion matrix &hellip; very close to the previous one obtained with the custom model.</p>
<h2 id="7conclusiona-idconclusiona">7.Conclusion<!-- raw HTML omitted --><!-- raw HTML omitted --></h2>
<p><strong>Comparing both models, one can see that the custom model is very close to the Sklearn model, ery similar results are observed:</strong></p>
<div class="highlight"><pre style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#8b008b;font-weight:bold">import</span> <span style="color:#008b45;text-decoration:underline">tabulate</span>
<span style="color:#8b008b;font-weight:bold">import</span> <span style="color:#008b45;text-decoration:underline">pandas</span> <span style="color:#8b008b;font-weight:bold">as</span> <span style="color:#008b45;text-decoration:underline">pd</span>
results = {
    <span style="color:#cd5555">&#39;Model&#39;</span>: [<span style="color:#cd5555">&#34;Custom AnN&#34;</span>, <span style="color:#cd5555">&#39;Sklearn ANN&#39;</span>],
    <span style="color:#cd5555">&#39;f1&#39;</span>: [<span style="color:#b452cd">91</span>, <span style="color:#b452cd">91.5</span>],
    <span style="color:#cd5555">&#39;precision&#39;</span>: [<span style="color:#b452cd">91.5</span>, <span style="color:#b452cd">91.5</span>],
    <span style="color:#cd5555">&#39;recall&#39;</span>: [<span style="color:#b452cd">91</span>, <span style="color:#b452cd">91.5</span>],
    <span style="color:#cd5555">&#39;error Type I&#39;</span>: [<span style="color:#cd5555">&#39;3.5%&#39;</span>, <span style="color:#cd5555">&#39;4.0%&#39;</span>],
    <span style="color:#cd5555">&#39;error Type II&#39;</span>: [<span style="color:#cd5555">&#39;5.5%&#39;</span>, <span style="color:#cd5555">&#39;4.5%&#39;</span>]}

df_results = pd.DataFrame(data=results).set_index(<span style="color:#cd5555">&#39;Model&#39;</span>)
<span style="color:#8b008b;font-weight:bold">print</span>(df_results.to_markdown())
</code></pre></div><pre><code>| Model       |   f1 |   precision |   recall | err Type I   | err Type II   |
|:------------|-----:|------------:|---------:|:-------------|:--------------|
| Custom AnN  | 91   |        91.5 |     91   | 3.5%         | 5.5%          |
| Sklearn ANN | 91.5 |        91.5 |     91.5 | 4.0%         | 4.5%          |
</code></pre>
<p><strong>On the other hand</strong>, the Sklearn model is a lot more faster in term of convergence, as it uses a faster and ADAM  optimizer (see table below)</p>
<div class="highlight"><pre style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">learning = {
    <span style="color:#cd5555">&#39;Model&#39;</span>: [<span style="color:#cd5555">&#34;Custom AnN&#34;</span>, <span style="color:#cd5555">&#39;Sklearn ANN&#39;</span>],
    <span style="color:#cd5555">&#39;lear iter&#39;</span>: [<span style="color:#b452cd">80000</span>, MLP_model.loss_curve_.index(MLP_model.loss_curve_[-<span style="color:#b452cd">1</span>])],
    <span style="color:#cd5555">&#39;final loss&#39;</span>: [<span style="color:#658b00">round</span>(loss[-<span style="color:#b452cd">1</span>],<span style="color:#b452cd">3</span>), <span style="color:#658b00">round</span>(MLP_model.loss_curve_[-<span style="color:#b452cd">1</span>], <span style="color:#b452cd">3</span>)]}

df_learning = pd.DataFrame(data=learning).set_index(<span style="color:#cd5555">&#39;Model&#39;</span>)
<span style="color:#8b008b;font-weight:bold">print</span>(df_learning.to_markdown())
</code></pre></div><pre><code>| Model       |   learn iter |   final loss |
|:------------|-------------:|-------------:|
| Custom AnN  |        80000 |        0.198 |
| Sklearn ANN |          217 |        0.221 |
</code></pre>
<p><strong>Now we have a custom Artificial Neural Network, built from scratch, with quiet good classification capabilities for all simple Xor-like datasets.<!-- raw HTML omitted --></strong></p>
<p><strong>Compared to the prebuilt model that already exists in Sklearn library, this custom model is not so fast and optimized&hellip; but that is a funny and good start exercise for a better background comprehension of artificial neural networks.</strong></p>
<p><strong>It is also a starting point from which we can improve and adapt this model to new issues! Ex: by adding more layers, implementing more robust optimizers, etc &hellip;</strong></p>
      
      <div class="related">

<h3>Similar articles:</h3>
<ul>
	
	<li><a href="https://domsdev.github.io/Data-science-blog/post/multiple_and_polynomial_regression_from_scratch/">Multiple and Polynomial Regression from scratch</a></li>
	
	<li><a href="https://domsdev.github.io/Data-science-blog/post/a_simple_linear_regression_from_scratch/">A Simple Linear Regression from scratch</a></li>
	
</ul>
</div>
      
    </div>
    
  </div>
</section>

    <script src="https://domsdev.github.io/Data-science-blog/js/copycode.js"></script>



<section class="section">
  <div class="container has-text-centered">
    <p>&copy; <a href="https://github.com/Domsdev/Data-science-blog/blob/main/MIT%20Licence.md">DomsDev</a></p>
    
      <p>Powered by <a href="https://gohugo.io/">Hugo</a> &amp; <a href="https://github.com/ribice/kiss">Kiss</a>.</p>
    
  </div>
</section>

<script type="text/javascript">
  var _paq = _paq || [];
  _paq.push(['trackPageView']);
  _paq.push(['enableLinkTracking']);
  (function() {
    var u="\/\/matomo.example.com\/";
    _paq.push(['setTrackerUrl', u+'piwik.php']);
    _paq.push(['setSiteId', '1']);
    var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
    g.type='text/javascript'; g.async=true; g.defer=true; g.src=u+'piwik.js'; s.parentNode.insertBefore(g,s);
  })();
</script>
<noscript>
  <img src="//matomo.example.com/piwik.php?idsite=1&amp;rec=1" style="border:0" alt="">
</noscript>

<script>
    (function(f, a, t, h, o, m){
        a[h]=a[h]||function(){
            (a[h].q=a[h].q||[]).push(arguments)
        };
        o=f.createElement('script'),
        m=f.getElementsByTagName('script')[0];
        o.async=1; o.src=t; o.id='fathom-script';
        m.parentNode.insertBefore(o,m)
    })(document, window, '\/\/fathom.example.com\/tracker.js', 'fathom');
    fathom('trackPageview');
</script>


</body>
</html>

<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

